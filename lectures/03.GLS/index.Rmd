---
title       : Generalized Least Squares
subtitle    : PCM Week 3
author      : Andrew Hipp (ahipp@mortonarb.org)
job         :
framework   : io2012        # {io2012, html5slides, shower, dzslides, ...}
highlighter : highlight.js  # {highlight.js, prettify, highlight}
hitheme     : tomorrow      #
widgets     : [mathjax]            # {mathjax, quiz, bootstrap}
mode        : selfcontained # {standalone, draft}
knit        : slidify::knit2slides
---

<style>
{
  background-color: #FFFFFF
}
</style>

# Remember that in phylogenetic independent contrasts (PIC), our goal was to render the tip states:
1. Independent
2. Equal variance (normalized by variance)

```{r, echo = FALSE, fig.width = 12, fig.cap = "Demonstration phylogeny, birth-death tree"}
library(ape)
library(geiger)
ntips2 = 5
node.cex = 1
tr <- structure(list(edge =
                    structure(c(9L, 9L, 8L, 8L, 7L, 7L, 6L, 6L, 3L, 4L, 1L, 2L, 8L, 9L, 7L, 5L), .Dim = c(8L, 2L),
                    .Dimnames = list(c("V1", "V2", "V3", "V4", "V5", "V6", "V7", "V8"), NULL)),
    edge.length = c(0.458192227, 0.458192227, 0.455310462, 0.455310462,
    0.2224209486, 0.2195391837, 0.6541055961, 1.331837007), Nnode = 4L,
    tip.label = c("s1", "s2", "s3", "s4", "s5"), node.label = 6:9),
    .Names = c("edge", "edge.length", "Nnode", "tip.label", "node.label"),
    class = "phylo", order = "postorder", seed = 3315337.05385891)


plotDemoTr <- function(x = NA) {
  plot(tr, cex = node.cex * 1.5)
  pp <- get("last_plot.phylo", envir=.PlotPhyloEnv)
  edgelabels(row.names(tr$edge), adj = c(0,-0.5), frame = 'n', cex = node.cex)
  nodesFont = rep(1, tr$Nnode)
  nodelabels(cex = node.cex, font = nodesFont, adj = -1.2, frame = 'n', text = tr$node.label)
}

plotDemoTr()

```

---
# We achieve these goals in two ways:
1. <b>Independence:</b> contrasts between the tips are independent of one another.
$$S1 - S2$$

2. <b>Equal variance:</b> dividing contrasts by the standard deviation (the square-root of the branch length separating tips) normalizes contrasts to unit variance
$${S1 - S2} \over \sqrt{V1 + V2}$$

```{r, echo = FALSE, fig.width = 12, fig.height = 5}
plot(tr)
edgelabels(round(tr$edge.lengdemo/layouts/assets/layouts/slide.htmlth, 3), adj = c(0.5, -0.5), frame = 'n')
```

---
# We have to do this because in ordinary least squares (OLS), data points are assumed constant and independent of one another. Thus, when we calculate $\beta$ using OLS, we assume that the covariance matrix is <b>I</b>, the identity matrix:

$$
  \begin{align}
    \beta
    & = (X'X)^{-1} X'y \\
    & = (X'IX)^{-1} X'Iy \\
  \end{align}  
$$

where, for our five-taxon tree:

$$
I =
\begin{bmatrix}
  1 & 0 & 0 & 0 & 0 \\
  0 & 1 & 0 & 0 & 0 \\
  0 & 0 & 1 & 0 & 0 \\
  0 & 0 & 0 & 1 & 0 \\
  0 & 0 & 0 & 0 & 1 \\
\end{bmatrix}
$$

This covariance matrix simply indicates that variances are equal for all five tips, and covariances are all zero.

---

# Now let's back up a minute and get our covariance matrix, <b>C</b>. Recall our tree... this time, we'll plot it with branch lengths instead of labels:

```{R, echo = FALSE, fig.width = 12}
plot(tr)
edgelabels(round(tr$edge.length, 3), adj = c(0.5, -0.5), frame = 'n')
```

---
# The covariance matrix takes the distance from the tips to the root as the expected variance; these go on the diagonals. The off-diagonals are then the expected covariance, estimated as the amount of shared evolutionary history between each pair of taxa

```{R, echo = FALSE, fig.width = 12, fig.height = 3}
      plot(tr)
      edgelabels(round(tr$edge.length, 3), adj = c(0.5, -0.5), frame = 'n')
```    

$$
  C =
      \begin{bmatrix}
        1.332 & 0.877 & 0.654 & 0.654 & 0 \\
        0.877 & 1.332 & 0.654 & 0.654 & 0 \\
        0.654 & 0.654 & 1.332 & 0.874 & 0 \\
        0.654 & 0.654 & 0.874 & 1.332 & 0 \\
        0 & 0 & 0 & 0 & 1.332 \\
      \end{bmatrix}
$$

---
# Inverting the covariance matrix (using `solve`) yields



$$
    C^{-1} =
    \begin{bmatrix}
      1.447 & -0.749 & -0.207 & -0.207 & 0 \\
      -0.749 & 1.447 & -0.207 & -0.207 & 0 \\
      -0.207 & -0.207 & 1.441 & -0.742 & 0 \\
      -0.207 & -0.207 & -0.742 & 1.441 & 0 \\
      0 & 0 & 0 & 0 & 0.751 \\
    \end{bmatrix}
$$

$C^{-1}$ has an interesting property. Where the column sums of $C$ estimate overall relatedness of each taxon to all others, the column sums of $C^{-1}$ estimate phylogenetic distinctiveness.

---
# Phylogenetic distinctiveness estimated as $C^{-1}$

```{r, fig.width = 12, fig.height = 6.5}
library(scales)
tr2 <- ladderize(sim.bdtree(n = 20)) # generate a 20 taxon birth-death tree
tr.w <- rescale(colSums(solve(vcv(tr2))), to = c(1, 4))
plot.phylo(tr2, show.tip.label = FALSE)
tiplabels(pch = 19, cex = tr.w, offset = 0.02)
```

---
# The covariance matrix $C$ is also used to obtain the phylogenetic mean, the ancestral character state under a Brownian motion model:

$$
\hat{a} = (1' C^{-1} 1)^{-1} (1' C^{-1} X)
$$

which is the same as the PIC estimate of the root of the tree. Note that the term $1' C^{-1}$ yields the column sums of $C^{-1}$; multiplying by the row vector $1$ yields the sum of the column sums. Placing this whole thing in the denominator normalizes the phylogenetic mean by $C$, intuitively satisfying given that $\hat{a}$ is really just a weighted mean.

# $C$ is also used to calculate the phylogenetic variance $\sigma^2$:

$$
  \sigma^2 = {{(x - \hat{a})' C^{-1} (x - \hat{a})} \over N}
$$

$$
  V = \sigma^2 C
$$

# and the likelihood, here represented in `R` notation

```
lnL <- dmvnorm(x, rep(a.hat, N), V, log = TRUE) # from mvtnorm package
```

---
# But we're getting a bit ahead of ourselves! These are all the special case of the no-predictor model, which is what we'll use when we are looking at stretching the tree. For purposes of this lecture, it's important to understand the special property of an inverted matrix: $C^{-1}$ has the desirable property of yielding <b>I</b> when it is multiplied by <b>C</b> ($C^{-1}C = I$).

```{r}
C = vcv(tr)
print(cbind(round(C, 3),
            round(solve(C), 3)
            )
      )
```

---
# ... and when we multiply these together:

```{r}
print(round(C %*% solve(C), 3))
```

---
# Incorporating $C$ into both the numerator and denominator of the least squares estimator generalizes that estimator to cases where $X$ and $Y$ are nonindependent with unequal variances. Thus, where the OLS estimator of $\beta$ was:

$$
  \begin{align}
    \beta & = (X'X)^{-1} X'y \\
    & = (X'IX)^{-1} X'Iy \\
  \end{align}  
$$

the GLS estimator of $\beta$ is:

$$
\beta = (X'CX)^{-1} X'Cy
$$

---
## OKAY!! Enough equations!! How do you actually do this stuff?!

There are a variety of generalized least squares (GLS) implementations floating around, but I'll present the one I've used the most, which comes from the `nlme` package.

Let's start by simulating some data and getting the package opened.

```{r}
library(geiger)
library(magrittr) # for formatting data
tr2 <- sim.bdtree(n = 50) # simulate a 50-tip birth-death bdtree...
# ... and two characters with r = 0.3, mean = 10
dat <- sim.char(tr2, par = matrix(c(1, 0.3, 0.3, 1), 2, 2), root = 10) %>%
  as.data.frame
names(dat) <- c('x', 'y')
```

---
# ... and let's plot it, just to get a sense of what the data look like.

```{r, fig.width = 12, fig.height = 5, fig.cap = "Biplot of raw data and independent contrasts"}
library(ggplot2, gridExtra)
dat.pic <- data.frame(x = pic(dat$x, tr2), y = pic(dat$y, tr2))
p1 <- qplot(x, y, data = dat.pic) + geom_smooth(method = 'lm') + ggtitle('Independent contrasts')
p2 <- qplot(x, y, data = dat) + geom_smooth(method = 'lm') + ggtitle('Raw data')
grid.arrange(p1, p2, nrow = 1)
```

---
# If we are willing to assume that the model of evolution is correct, that these traits actually evolved according to a Brownian motion model, we can just use GLS without adjusting the branch lengths or modifying $C$ in any way. This is equivalent to PIC on an untransformed tree:

```{r}
library(nlme, ape)
dat.fits <- list(
  gls = gls(y ~ x, data = dat, correlation = corBrownian(value = 1, phy = tr2)),
  pic = lm(y ~ x + 0, data = dat.pic),
  ols = lm(y ~ x, data = dat)
  )
dat.fits$pic$coefficients = c(NA, dat.fits$pic$coefficients)
sapply(dat.fits, coef)
```

---
# We can get more information from each of these analyses using the `summary` function, which has a separate method for an `lm` object than for a `gls` object.

```{r}
summary(dat.fits$gls)
```

---
# But it may well be that a Brownian motion model doesn't fit our data. One of the lessons of Revell 2010 and Rohlf 2006 is that GLS is BLUE (best linear unbiased estimator), but only if we specify the correlation structure correctly. Let's plot the log-likelihood for our original data, which were evolved on the tree, and one for which the data were not evolved on the tree.

```{r}
tr3 <- tr2
tr3$tip.label <- sample(tr3$tip.label) # randomizes tip label order
lambdaVals <- seq(from = 0, to = 1, by = 0.02)
## create rescaled trees from 0 to 1, where 0 is no phylogenetic structure, 1 is original:
tr2.set <- lapply(lambdaVals,
                 function(x) geiger:::rescale.phylo(tr2, 'lambda', x))
tr3.set <- lapply(lambdaVals,
                 function(x) geiger:::rescale.phylo(tr3, 'lambda', x))
names(tr2.set) <- names(tr3.set) <- as.character(lambdaVals)
## and now fit all the models:
dat.fits2 <- list(
                   gls = lapply(tr2.set, function(trInd)
                     gls(y ~ x, data = dat, correlation = corBrownian(value = 1, phy = trInd))),
                   gls.rnd = lapply(tr3.set, function(trInd)
                     gls(y ~ x, data = dat, correlation = corBrownian(value = 1, phy = trInd)))
                   )
```

---
# So you have some perspective on what we've just done, here are three example trees, scaled by Pagel's $\lambda$

```{r, fig.width = 12, fig.height = 6}
layout(matrix(1:3, 1))
for(i in c("0", "0.5", "1")) plot(tr2.set[[i]], show.tip.label = F, main = paste('lambda =', i))
```

---
# Now let's put the results into a couple of data frames, so we can plot lnL against Pagel's $\lambda$. Here we have the log-likelihood plot with a dashed line at $lnL - 2$ for the raw data... this is a commonly used value that approximates the 95% confidence interval when sample sizes are large enough.

```{r, fig.width = 12, fig.height = 3}
tr2.lnL <- data.frame(lambda = lambdaVals, lnL = sapply(dat.fits2$gls, logLik))
tr3.lnL <- data.frame(lambda = lambdaVals, lnL = sapply(dat.fits2$gls.rnd, logLik))
layout(matrix(1:2, 1))
plot(tr2.lnL,  type = 'l', main = "Original data, real lambda = 1")
abline(h = max(tr2.lnL$lnL) - 2, lty = 'dashed')
plot(tr3.lnL,  type = 'l', main = "Shuffled data, real lambda << 1")
abline(h = max(tr3.lnL$lnL) - 2, lty = 'dashed')
```

---
# When you are doing GLS regression in real time, you can simultaneously fit the model using generalized least squares, and Pagel's $\lambda$ or any other tree scalar using maximum likelihood. Likelihood plots are helpful for seeing what's really going on with your data, but `R` will help you fit these models more smoothly. Let's look at tr3, because that's the one where assuming Brownian motion would be wrong.

```{r}
# first fit the model with just a Brownian assumption
trs.glsAlt <- list(
  brown.rnd = gls(y ~ x, data = dat, correlation = corBrownian(phy = tr3)),
  pagel.rnd = gls(y ~ x, data = dat, correlation = corPagel(value = 1, phy = tr3)),
  brown.raw = gls(y ~ x, data = dat, correlation = corBrownian(phy = tr2)),
  pagel.raw = gls(y ~ x, data = dat, correlation = corPagel(value = 1, phy = tr2))
  )
```
Now, let's look at the results.

---
# first, the coefficients. Notice that for the raw data, both the Brownian assumption (traditional GLS / PIC) and the model fitting Pagel's $\lambda$ give approximately the same results:

```{r}
sapply(trs.glsAlt, coef) %>% t
```

# When we look at the value of Pagel's $\lambda$ for each model, we can see why:

```{r}
paste('shuffled data lambda =', trs.glsAlt$pagel.rnd$modelStruct,
      '\nraw data lambda =', trs.glsAlt$pagel.raw$modelStruct) %>%
message
```

---
## Now you get to try it yourselves! Let's discuss the articles, then you can work on the tutorial.
