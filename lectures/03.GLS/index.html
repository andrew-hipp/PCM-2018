<!DOCTYPE html>
<html>
<head>
  <title>Generalized Least Squares</title>
  <meta charset="utf-8">
  <meta name="description" content="Generalized Least Squares">
  <meta name="author" content="Andrew Hipp (ahipp@mortonarb.org)">
  <meta name="generator" content="slidify" />
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta http-equiv="X-UA-Compatible" content="chrome=1">
  <link rel="stylesheet" href="libraries/frameworks/io2012/css/default.css" media="all" >
  <link rel="stylesheet" href="libraries/frameworks/io2012/css/phone.css" 
    media="only screen and (max-device-width: 480px)" >
  <link rel="stylesheet" href="libraries/frameworks/io2012/css/slidify.css" >
  <link rel="stylesheet" href="libraries/highlighters/highlight.js/css/tomorrow.css" />
  <base target="_blank"> <!-- This amazingness opens all links in a new tab. -->  <link rel=stylesheet href="./assets/css/ribbons.css"></link>

  
  <!-- Grab CDN jQuery, fall back to local if offline -->
  <script src="http://ajax.aspnetcdn.com/ajax/jQuery/jquery-1.7.min.js"></script>
  <script>window.jQuery || document.write('<script src="libraries/widgets/quiz/js/jquery.js"><\/script>')</script> 
  <script data-main="libraries/frameworks/io2012/js/slides" 
    src="libraries/frameworks/io2012/js/require-1.0.8.min.js">
  </script>
  
  

</head>
<body style="opacity: 0">
  <slides class="layout-widescreen">
    
    <!-- LOGO SLIDE -->
        <slide class="title-slide segue nobackground">
  <hgroup class="auto-fadein">
    <h1>Generalized Least Squares</h1>
    <h2>PCM Week 3</h2>
    <p>Andrew Hipp (ahipp@mortonarb.org)<br/></p>
  </hgroup>
  <article></article>  
</slide>
    

    <!-- SLIDES -->
    <slide class="" id="slide-1" style="background:;">
  <article data-timings="">
    <style>
{
  background-color: #FFFFFF
}
</style>

<h1>Remember that in phylogenetic independent contrasts (PIC), our goal was to render the tip states:</h1>

<ol>
<li>Independent</li>
<li>Equal variance (normalized by variance)</li>
</ol>

<p><img src="assets/fig/unnamed-chunk-1-1.png" alt="Demonstration phylogeny, birth-death tree"></p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-2" style="background:;">
  <hgroup>
    <h1>We achieve these goals in two ways:</h1>
  </hgroup>
  <article data-timings="">
    <ol>
<li><p><b>Independence:</b> contrasts between the tips are independent of one another.
\[S1 - S2\]</p></li>
<li><p><b>Equal variance:</b> dividing contrasts by the standard deviation (the square-root of the branch length separating tips) normalizes contrasts to unit variance
\[{S1 - S2} \over \sqrt{V1 + V2}\]</p></li>
</ol>

<p><img src="assets/fig/unnamed-chunk-2-1.png" alt="plot of chunk unnamed-chunk-2"></p>

<pre><code>## Error in BOTHlabels(text, sel, XX, YY, adj, frame, pch, thermo, pie, piecol, : object &#39;layouts&#39; not found
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-3" style="background:;">
  <hgroup>
    <h1>We have to do this because in ordinary least squares (OLS), data points are assumed constant and independent of one another. Thus, when we calculate \(\beta\) using OLS, we assume that the covariance matrix is <b>I</b>, the identity matrix:</h1>
  </hgroup>
  <article data-timings="">
    <p>\[
  \begin{align}
    \beta
    & = (X'X)^{-1} X'y \\
    & = (X'IX)^{-1} X'Iy \\
  \end{align}  
\]</p>

<p>where, for our five-taxon tree:</p>

<p>\[
I =
\begin{bmatrix}
  1 & 0 & 0 & 0 & 0 \\
  0 & 1 & 0 & 0 & 0 \\
  0 & 0 & 1 & 0 & 0 \\
  0 & 0 & 0 & 1 & 0 \\
  0 & 0 & 0 & 0 & 1 \\
\end{bmatrix}
\]</p>

<p>This covariance matrix simply indicates that variances are equal for all five tips, and covariances are all zero.</p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-4" style="background:;">
  <hgroup>
    <h1>Now let&#39;s back up a minute and get our covariance matrix, <b>C</b>. Recall our tree... this time, we&#39;ll plot it with branch lengths instead of labels:</h1>
  </hgroup>
  <article data-timings="">
    <p><img src="assets/fig/unnamed-chunk-3-1.png" alt="plot of chunk unnamed-chunk-3"></p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-5" style="background:;">
  <hgroup>
    <h1>The covariance matrix takes the distance from the tips to the root as the expected variance; these go on the diagonals. The off-diagonals are then the expected covariance, estimated as the amount of shared evolutionary history between each pair of taxa</h1>
  </hgroup>
  <article data-timings="">
    <p><img src="assets/fig/unnamed-chunk-4-1.png" alt="plot of chunk unnamed-chunk-4"></p>

<p>\[
  C =
      \begin{bmatrix}
        1.332 & 0.877 & 0.654 & 0.654 & 0 \\
        0.877 & 1.332 & 0.654 & 0.654 & 0 \\
        0.654 & 0.654 & 1.332 & 0.874 & 0 \\
        0.654 & 0.654 & 0.874 & 1.332 & 0 \\
        0 & 0 & 0 & 0 & 1.332 \\
      \end{bmatrix}
\]</p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-6" style="background:;">
  <hgroup>
    <h1>Inverting the covariance matrix (using <code>solve</code>) yields</h1>
  </hgroup>
  <article data-timings="">
    <p>\[
    C^{-1} =
    \begin{bmatrix}
      1.447 & -0.749 & -0.207 & -0.207 & 0 \\
      -0.749 & 1.447 & -0.207 & -0.207 & 0 \\
      -0.207 & -0.207 & 1.441 & -0.742 & 0 \\
      -0.207 & -0.207 & -0.742 & 1.441 & 0 \\
      0 & 0 & 0 & 0 & 0.751 \\
    \end{bmatrix}
\]</p>

<p>\(C^{-1}\) has an interesting property. Where the column sums of \(C\) estimate overall relatedness of each taxon to all others, the column sums of \(C^{-1}\) estimate phylogenetic distinctiveness.</p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-7" style="background:;">
  <hgroup>
    <h1>Phylogenetic distinctiveness estimated as \(C^{-1}\)</h1>
  </hgroup>
  <article data-timings="">
    <pre><code class="r">library(scales)
tr2 &lt;- ladderize(sim.bdtree(n = 20)) # generate a 20 taxon birth-death tree
tr.w &lt;- rescale(colSums(solve(vcv(tr2))), to = c(1, 4))
plot.phylo(tr2, show.tip.label = FALSE)
tiplabels(pch = 19, cex = tr.w, offset = 0.02)
</code></pre>

<p><img src="assets/fig/unnamed-chunk-5-1.png" alt="plot of chunk unnamed-chunk-5"></p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-8" style="background:;">
  <hgroup>
    <h1>The covariance matrix \(C\) is also used to obtain the phylogenetic mean, the ancestral character state under a Brownian motion model:</h1>
  </hgroup>
  <article data-timings="">
    <p>\[
\hat{a} = (1' C^{-1} 1)^{-1} (1' C^{-1} X)
\]</p>

<p>which is the same as the PIC estimate of the root of the tree. Note that the term \(1' C^{-1}\) yields the column sums of \(C^{-1}\); multiplying by the row vector \(1\) yields the sum of the column sums. Placing this whole thing in the denominator normalizes the phylogenetic mean by \(C\), intuitively satisfying given that \(\hat{a}\) is really just a weighted mean.</p>

<h1>\(C\) is also used to calculate the phylogenetic variance \(\sigma^2\):</h1>

<p>\[
  \sigma^2 = {{(x - \hat{a})' C^{-1} (x - \hat{a})} \over N}
\]</p>

<p>\[
  V = \sigma^2 C
\]</p>

<h1>and the likelihood, here represented in <code>R</code> notation</h1>

<pre><code>lnL &lt;- dmvnorm(x, rep(a.hat, N), V, log = TRUE) # from mvtnorm package
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-9" style="background:;">
  <hgroup>
    <h1>But we&#39;re getting a bit ahead of ourselves! These are all the special case of the no-predictor model, which is what we&#39;ll use when we are looking at stretching the tree. For purposes of this lecture, it&#39;s important to understand the special property of an inverted matrix: \(C^{-1}\) has the desirable property of yielding <b>I</b> when it is multiplied by <b>C</b> (\(C^{-1}C = I\)).</h1>
  </hgroup>
  <article data-timings="">
    <pre><code class="r">C = vcv(tr)
print(cbind(round(C, 3),
            round(solve(C), 3)
            )
      )
</code></pre>

<pre><code>##       s1    s2    s3    s4    s5     s1     s2     s3     s4    s5
## s1 1.332 0.877 0.654 0.654 0.000  1.447 -0.749 -0.207 -0.207 0.000
## s2 0.877 1.332 0.654 0.654 0.000 -0.749  1.447 -0.207 -0.207 0.000
## s3 0.654 0.654 1.332 0.874 0.000 -0.207 -0.207  1.441 -0.742 0.000
## s4 0.654 0.654 0.874 1.332 0.000 -0.207 -0.207 -0.742  1.441 0.000
## s5 0.000 0.000 0.000 0.000 1.332  0.000  0.000  0.000  0.000 0.751
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-10" style="background:;">
  <hgroup>
    <h1>... and when we multiply these together:</h1>
  </hgroup>
  <article data-timings="">
    <pre><code class="r">print(round(C %*% solve(C), 3))
</code></pre>

<pre><code>##    s1 s2 s3 s4 s5
## s1  1  0  0  0  0
## s2  0  1  0  0  0
## s3  0  0  1  0  0
## s4  0  0  0  1  0
## s5  0  0  0  0  1
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-11" style="background:;">
  <hgroup>
    <h1>Incorporating \(C\) into both the numerator and denominator of the least squares estimator generalizes that estimator to cases where \(X\) and \(Y\) are nonindependent with unequal variances. Thus, where the OLS estimator of \(\beta\) was:</h1>
  </hgroup>
  <article data-timings="">
    <p>\[
  \begin{align}
    \beta & = (X'X)^{-1} X'y \\
    & = (X'IX)^{-1} X'Iy \\
  \end{align}  
\]</p>

<p>the GLS estimator of \(\beta\) is:</p>

<p>\[
\beta = (X'CX)^{-1} X'Cy
\]</p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-12" style="background:;">
  <hgroup>
    <h2>OKAY!! Enough equations!! How do you actually do this stuff?!</h2>
  </hgroup>
  <article data-timings="">
    <p>There are a variety of generalized least squares (GLS) implementations floating around, but I&#39;ll present the one I&#39;ve used the most, which comes from the <code>nlme</code> package.</p>

<p>Let&#39;s start by simulating some data and getting the package opened.</p>

<pre><code class="r">library(geiger)
library(magrittr) # for formatting data
tr2 &lt;- sim.bdtree(n = 50) # simulate a 50-tip birth-death bdtree...
# ... and two characters with r = 0.3, mean = 10
dat &lt;- sim.char(tr2, par = matrix(c(1, 0.3, 0.3, 1), 2, 2), root = 10) %&gt;%
  as.data.frame
names(dat) &lt;- c(&#39;x&#39;, &#39;y&#39;)
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-13" style="background:;">
  <hgroup>
    <h1>... and let&#39;s plot it, just to get a sense of what the data look like.</h1>
  </hgroup>
  <article data-timings="">
    <pre><code class="r">library(ggplot2, gridExtra)
dat.pic &lt;- data.frame(x = pic(dat$x, tr2), y = pic(dat$y, tr2))
p1 &lt;- qplot(x, y, data = dat.pic) + geom_smooth(method = &#39;lm&#39;) + ggtitle(&#39;Independent contrasts&#39;)
p2 &lt;- qplot(x, y, data = dat) + geom_smooth(method = &#39;lm&#39;) + ggtitle(&#39;Raw data&#39;)
grid.arrange(p1, p2, nrow = 1)
</code></pre>

<p><img src="assets/fig/unnamed-chunk-9-1.png" alt="Biplot of raw data and independent contrasts"></p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-14" style="background:;">
  <hgroup>
    <h1>If we are willing to assume that the model of evolution is correct, that these traits actually evolved according to a Brownian motion model, we can just use GLS without adjusting the branch lengths or modifying \(C\) in any way. This is equivalent to PIC on an untransformed tree:</h1>
  </hgroup>
  <article data-timings="">
    <pre><code class="r">library(nlme, ape)
dat.fits &lt;- list(
  gls = gls(y ~ x, data = dat, correlation = corBrownian(value = 1, phy = tr2)),
  pic = lm(y ~ x + 0, data = dat.pic),
  ols = lm(y ~ x, data = dat)
  )
dat.fits$pic$coefficients = c(NA, dat.fits$pic$coefficients)
sapply(dat.fits, coef)
</code></pre>

<pre><code>##                   gls       pic       ols
## (Intercept) 7.7272827        NA 8.9455430
## x           0.2323257 0.2703705 0.1182874
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-15" style="background:;">
  <hgroup>
    <h1>We can get more information from each of these analyses using the <code>summary</code> function, which has a separate method for an <code>lm</code> object than for a <code>gls</code> object.</h1>
  </hgroup>
  <article data-timings="">
    <pre><code class="r">summary(dat.fits$gls)
</code></pre>

<pre><code>## Generalized least squares fit by REML
##   Model: y ~ x 
##   Data: dat 
##        AIC      BIC    logLik
##   138.8839 144.4975 -66.44196
## 
## Correlation Structure: corBrownian
##  Formula: ~1 
##  Parameter estimate(s):
## numeric(0)
## 
## Coefficients:
##                Value Std.Error  t-value p-value
## (Intercept) 7.727283  1.555181 4.968736   0.000
## x           0.232326  0.134704 1.724713   0.091
## 
##  Correlation: 
##   (Intr)
## x -0.919
## 
## Standardized residuals:
##         Min          Q1         Med          Q3         Max 
## -1.99594091 -0.39455711 -0.07507029  0.38367479  1.75896130 
## 
## Residual standard error: 1.63941 
## Degrees of freedom: 50 total; 48 residual
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-16" style="background:;">
  <hgroup>
    <h1>But it may well be that a Brownian motion model doesn&#39;t fit our data. One of the lessons of Revell 2010 and Rohlf 2006 is that GLS is BLUE (best linear unbiased estimator), but only if we specify the correlation structure correctly. Let&#39;s plot the log-likelihood for our original data, which were evolved on the tree, and one for which the data were not evolved on the tree.</h1>
  </hgroup>
  <article data-timings="">
    <pre><code class="r">tr3 &lt;- tr2
tr3$tip.label &lt;- sample(tr3$tip.label) # randomizes tip label order
lambdaVals &lt;- seq(from = 0, to = 1, by = 0.02)
## create rescaled trees from 0 to 1, where 0 is no phylogenetic structure, 1 is original:
tr2.set &lt;- lapply(lambdaVals,
                 function(x) geiger:::rescale.phylo(tr2, &#39;lambda&#39;, x))
tr3.set &lt;- lapply(lambdaVals,
                 function(x) geiger:::rescale.phylo(tr3, &#39;lambda&#39;, x))
names(tr2.set) &lt;- names(tr3.set) &lt;- as.character(lambdaVals)
## and now fit all the models:
dat.fits2 &lt;- list(
                   gls = lapply(tr2.set, function(trInd)
                     gls(y ~ x, data = dat, correlation = corBrownian(value = 1, phy = trInd))),
                   gls.rnd = lapply(tr3.set, function(trInd)
                     gls(y ~ x, data = dat, correlation = corBrownian(value = 1, phy = trInd)))
                   )
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-17" style="background:;">
  <hgroup>
    <h1>So you have some perspective on what we&#39;ve just done, here are three example trees, scaled by Pagel&#39;s \(\lambda\)</h1>
  </hgroup>
  <article data-timings="">
    <pre><code class="r">layout(matrix(1:3, 1))
for(i in c(&quot;0&quot;, &quot;0.5&quot;, &quot;1&quot;)) plot(tr2.set[[i]], show.tip.label = F, main = paste(&#39;lambda =&#39;, i))
</code></pre>

<p><img src="assets/fig/unnamed-chunk-13-1.png" alt="plot of chunk unnamed-chunk-13"></p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-18" style="background:;">
  <hgroup>
    <h1>Now let&#39;s put the results into a couple of data frames, so we can plot lnL against Pagel&#39;s \(\lambda\). Here we have the log-likelihood plot with a dashed line at \(lnL - 2\) for the raw data... this is a commonly used value that approximates the 95% confidence interval when sample sizes are large enough.</h1>
  </hgroup>
  <article data-timings="">
    <pre><code class="r">tr2.lnL &lt;- data.frame(lambda = lambdaVals, lnL = sapply(dat.fits2$gls, logLik))
tr3.lnL &lt;- data.frame(lambda = lambdaVals, lnL = sapply(dat.fits2$gls.rnd, logLik))
layout(matrix(1:2, 1))
plot(tr2.lnL,  type = &#39;l&#39;, main = &quot;Original data, real lambda = 1&quot;)
abline(h = max(tr2.lnL$lnL) - 2, lty = &#39;dashed&#39;)
plot(tr3.lnL,  type = &#39;l&#39;, main = &quot;Shuffled data, real lambda &lt;&lt; 1&quot;)
abline(h = max(tr3.lnL$lnL) - 2, lty = &#39;dashed&#39;)
</code></pre>

<p><img src="assets/fig/unnamed-chunk-14-1.png" alt="plot of chunk unnamed-chunk-14"></p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-19" style="background:;">
  <hgroup>
    <h1>When you are doing GLS regression in real time, you can simultaneously fit the model using generalized least squares, and Pagel&#39;s \(\lambda\) or any other tree scalar using maximum likelihood. Likelihood plots are helpful for seeing what&#39;s really going on with your data, but <code>R</code> will help you fit these models more smoothly. Let&#39;s look at tr3, because that&#39;s the one where assuming Brownian motion would be wrong.</h1>
  </hgroup>
  <article data-timings="">
    <pre><code class="r"># first fit the model with just a Brownian assumption
trs.glsAlt &lt;- list(
  brown.rnd = gls(y ~ x, data = dat, correlation = corBrownian(phy = tr3)),
  pagel.rnd = gls(y ~ x, data = dat, correlation = corPagel(value = 1, phy = tr3)),
  brown.raw = gls(y ~ x, data = dat, correlation = corBrownian(phy = tr2)),
  pagel.raw = gls(y ~ x, data = dat, correlation = corPagel(value = 1, phy = tr2))
  )
</code></pre>

<p>Now, let&#39;s look at the results.</p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-20" style="background:;">
  <hgroup>
    <h1>first, the coefficients. Notice that for the raw data, both the Brownian assumption (traditional GLS / PIC) and the model fitting Pagel&#39;s \(\lambda\) give approximately the same results:</h1>
  </hgroup>
  <article data-timings="">
    <pre><code class="r">sapply(trs.glsAlt, coef) %&gt;% t
</code></pre>

<pre><code>##           (Intercept)          x
## brown.rnd    8.124274 0.20963551
## pagel.rnd   10.180093 0.03557905
## brown.raw    7.727283 0.23232574
## pagel.raw    7.729468 0.23211213
</code></pre>

<h1>When we look at the value of Pagel&#39;s \(\lambda\) for each model, we can see why:</h1>

<pre><code class="r">paste(&#39;shuffled data lambda =&#39;, trs.glsAlt$pagel.rnd$modelStruct,
      &#39;\nraw data lambda =&#39;, trs.glsAlt$pagel.raw$modelStruct) %&gt;%
message
</code></pre>

<pre><code>## shuffled data lambda = -0.307742864461561 
## raw data lambda = 1.00056334390611
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-21" style="background:;">
  <hgroup>
    <h2>Now you get to try it yourselves! Let&#39;s discuss the articles, then you can work on the tutorial.</h2>
  </hgroup>
  <article data-timings="">
    
  </article>
  <!-- Presenter Notes -->
</slide>

    <slide class="backdrop"></slide>
  </slides>
  <div class="pagination pagination-small" id='io2012-ptoc' style="display:none;">
    <ul>
      <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=1 title='NA'>
         1
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=2 title='We achieve these goals in two ways:'>
         2
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=3 title='We have to do this because in ordinary least squares (OLS), data points are assumed constant and independent of one another. Thus, when we calculate \(\beta\) using OLS, we assume that the covariance matrix is <b>I</b>, the identity matrix:'>
         3
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=4 title='Now let&#39;s back up a minute and get our covariance matrix, <b>C</b>. Recall our tree... this time, we&#39;ll plot it with branch lengths instead of labels:'>
         4
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=5 title='The covariance matrix takes the distance from the tips to the root as the expected variance; these go on the diagonals. The off-diagonals are then the expected covariance, estimated as the amount of shared evolutionary history between each pair of taxa'>
         5
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=6 title='Inverting the covariance matrix (using <code>solve</code>) yields'>
         6
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=7 title='Phylogenetic distinctiveness estimated as \(C^{-1}\)'>
         7
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=8 title='The covariance matrix \(C\) is also used to obtain the phylogenetic mean, the ancestral character state under a Brownian motion model:'>
         8
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=9 title='But we&#39;re getting a bit ahead of ourselves! These are all the special case of the no-predictor model, which is what we&#39;ll use when we are looking at stretching the tree. For purposes of this lecture, it&#39;s important to understand the special property of an inverted matrix: \(C^{-1}\) has the desirable property of yielding <b>I</b> when it is multiplied by <b>C</b> (\(C^{-1}C = I\)).'>
         9
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=10 title='... and when we multiply these together:'>
         10
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=11 title='Incorporating \(C\) into both the numerator and denominator of the least squares estimator generalizes that estimator to cases where \(X\) and \(Y\) are nonindependent with unequal variances. Thus, where the OLS estimator of \(\beta\) was:'>
         11
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=12 title='OKAY!! Enough equations!! How do you actually do this stuff?!'>
         12
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=13 title='... and let&#39;s plot it, just to get a sense of what the data look like.'>
         13
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=14 title='If we are willing to assume that the model of evolution is correct, that these traits actually evolved according to a Brownian motion model, we can just use GLS without adjusting the branch lengths or modifying \(C\) in any way. This is equivalent to PIC on an untransformed tree:'>
         14
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=15 title='We can get more information from each of these analyses using the <code>summary</code> function, which has a separate method for an <code>lm</code> object than for a <code>gls</code> object.'>
         15
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=16 title='But it may well be that a Brownian motion model doesn&#39;t fit our data. One of the lessons of Revell 2010 and Rohlf 2006 is that GLS is BLUE (best linear unbiased estimator), but only if we specify the correlation structure correctly. Let&#39;s plot the log-likelihood for our original data, which were evolved on the tree, and one for which the data were not evolved on the tree.'>
         16
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=17 title='So you have some perspective on what we&#39;ve just done, here are three example trees, scaled by Pagel&#39;s \(\lambda\)'>
         17
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=18 title='Now let&#39;s put the results into a couple of data frames, so we can plot lnL against Pagel&#39;s \(\lambda\). Here we have the log-likelihood plot with a dashed line at \(lnL - 2\) for the raw data... this is a commonly used value that approximates the 95% confidence interval when sample sizes are large enough.'>
         18
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=19 title='When you are doing GLS regression in real time, you can simultaneously fit the model using generalized least squares, and Pagel&#39;s \(\lambda\) or any other tree scalar using maximum likelihood. Likelihood plots are helpful for seeing what&#39;s really going on with your data, but <code>R</code> will help you fit these models more smoothly. Let&#39;s look at tr3, because that&#39;s the one where assuming Brownian motion would be wrong.'>
         19
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=20 title='first, the coefficients. Notice that for the raw data, both the Brownian assumption (traditional GLS / PIC) and the model fitting Pagel&#39;s \(\lambda\) give approximately the same results:'>
         20
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=21 title='Now you get to try it yourselves! Let&#39;s discuss the articles, then you can work on the tutorial.'>
         21
      </a>
    </li>
  </ul>
  </div>  <!--[if IE]>
    <script 
      src="http://ajax.googleapis.com/ajax/libs/chrome-frame/1/CFInstall.min.js">  
    </script>
    <script>CFInstall.check({mode: 'overlay'});</script>
  <![endif]-->
</body>
  <!-- Load Javascripts for Widgets -->
  
  <!-- MathJax: Fall back to local if CDN offline but local image fonts are not supported (saves >100MB) -->
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [['$','$'], ['\\(','\\)']],
        processEscapes: true
      }
    });
  </script>
  <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/2.0-latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  <!-- <script src="https://c328740.ssl.cf1.rackcdn.com/mathjax/2.0-latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  </script> -->
  <script>window.MathJax || document.write('<script type="text/x-mathjax-config">MathJax.Hub.Config({"HTML-CSS":{imageFont:null}});<\/script><script src="libraries/widgets/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"><\/script>')
</script>
<!-- LOAD HIGHLIGHTER JS FILES -->
  <script src="libraries/highlighters/highlight.js/highlight.pack.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>
  <!-- DONE LOADING HIGHLIGHTER JS FILES -->
   
  </html>